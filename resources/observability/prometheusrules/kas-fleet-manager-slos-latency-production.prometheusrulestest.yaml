---
$schema: /app-interface/prometheus-rule-test-1.yml

rule_files:
  - /observability/prometheusrules/kas-fleet-manager-slos-latency-production.prometheusrules.yaml

evaluation_interval: 1m

tests:
  - interval: 1m
    input_series:
      # 0m  - 30m   all good
      # 30m - 60m   5% of requests degraded to 2le bucket
      # 60m - 90m   10% of requests degraded to 2le bucket
      - series: 'api_inbound_request_duration_bucket{job="kas-fleet-manager-metrics",namespace="managed-services-production",le="1",code="200"}'
        values: '0+1x30   31+19x30  601+18x30'
      - series: 'api_inbound_request_duration_bucket{job="kas-fleet-manager-metrics",namespace="managed-services-production",le="2",code="200"}'
        values: '0+1x30   31+1x30   61+2x30'
      - series: 'api_inbound_request_duration_count{job="kas-fleet-manager-metrics",namespace="managed-services-production"}'
        values: '0+1x30   31+20x30  631+20x30'
    alert_rule_test:
      - eval_time: 30m
        alertname: KasFleetManagerAPILatency30mto6hP99BudgetBurn
        exp_alerts: [ ]
      - eval_time: 60m
        alertname: KasFleetManagerAPILatency30mto6hP99BudgetBurn
        exp_alerts: [ ]
      - eval_time: 90m
        alertname: KasFleetManagerAPILatency30mto6hP99BudgetBurn
        exp_alerts:
          - exp_labels:
              alertname: KasFleetManagerAPILatency30mto6hP99BudgetBurn
              job: kas-fleet-manager-metrics
              latency: 1
              namespace: managed-services-production
              quantile: 99
              service: kas-fleet-manager
              severity: critical
              team: appsre
            exp_annotations:
              dashboard: "https://grafana.app-sre.devshift.net/d/Tbw1EgoMz/kas-fleet-manager-slos?orgId=1&var-datasource=app-sre-prod-04-prometheus"
              message: "High 6h requests latency budget burn for Kas Fleet Manager API p99 (current value: 71.73m)"
              runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/managed-services/sop#kafka-service-fleet-manager-latency"
  - interval: 1m
    input_series:
      # 0m    - 60m   all good
      # 60m   - 120m  1% of requests degraded to 2le bucket
      # 120m  - 150m  3% of requests degraded to 2le bucket
      - series: 'api_inbound_request_duration_bucket{job="kas-fleet-manager-metrics",namespace="managed-services-production",le="1",code="200"}'
        values: '0+1x60   61+99x60    6001+97x30'
      - series: 'api_inbound_request_duration_bucket{job="kas-fleet-manager-metrics",namespace="managed-services-production",le="2",code="200"}'
        values: '0+1x60   61+1x60     121+3x30'
      - series: 'api_inbound_request_duration_count{job="kas-fleet-manager-metrics",namespace="managed-services-production"}'
        values: '0+1x60   61+100x60   6061+100x30'
    alert_rule_test:
      - eval_time: 60m
        alertname: KasFleetManagerAPILatency2hto1dor6hto3dP99BudgetBurn
        exp_alerts: [ ]
      - eval_time: 120m
        alertname: KasFleetManagerAPILatency2hto1dor6hto3dP99BudgetBurn
        exp_alerts: [ ]
      - eval_time: 150m
        alertname: KasFleetManagerAPILatency2hto1dor6hto3dP99BudgetBurn
        exp_alerts:
          - exp_labels:
              alertname: KasFleetManagerAPILatency2hto1dor6hto3dP99BudgetBurn
              job: kas-fleet-manager-metrics
              latency: 1
              namespace: managed-services-production
              quantile: 99
              service: kas-fleet-manager
              severity: high
            exp_annotations:
              dashboard: "https://grafana.app-sre.devshift.net/d/Tbw1EgoMz/kas-fleet-manager-slos?orgId=1&var-datasource=app-sre-prod-04-prometheus"
              message: "High 1d/3d requests latency budget burn for Kas Fleet Manager API p99 (current value: 16.09m)"
              runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/managed-services/sop#kafka-service-fleet-manager-latency"
  - interval: 1m
    input_series:
      # 0m  - 30m   all good
      # 30m - 60m   50% of requests degraded to 1le bucket
      # 60m - 90m   70% of requests degraded to 1le bucket
      - series: 'api_inbound_request_duration_bucket{job="kas-fleet-manager-metrics",namespace="managed-services-production",le="0.1",code="200"}'
        values: '0+1x30   31+5x30   181+3x60'
      - series: 'api_inbound_request_duration_bucket{job="kas-fleet-manager-metrics",namespace="managed-services-production",le="1",code="200"}'
        values: '0+1x30   31+5x30   181+7x60'
      - series: 'api_inbound_request_duration_count{job="kas-fleet-manager-metrics",namespace="managed-services-production"}'
        values: '0+1x30   31+10x30  331+10x60'
    alert_rule_test:
      - eval_time: 30m
        alertname: KasFleetManagerAPILatency30mto6hP90BudgetBurn
        exp_alerts: [ ]
      - eval_time: 60m
        alertname: KasFleetManagerAPILatency30mto6hP90BudgetBurn
        exp_alerts: [ ]
      - eval_time: 120m
        alertname: KasFleetManagerAPILatency30mto6hP90BudgetBurn
        exp_alerts:
          - exp_labels:
              alertname: KasFleetManagerAPILatency30mto6hP90BudgetBurn
              job: kas-fleet-manager-metrics
              latency: 0.1
              namespace: managed-services-production
              quantile: 90
              service: kas-fleet-manager
              severity: critical
              team: appsre
            exp_annotations:
              dashboard: "https://grafana.app-sre.devshift.net/d/Tbw1EgoMz/kas-fleet-manager-slos?orgId=1&var-datasource=app-sre-prod-04-prometheus"
              message: "High 1h/6h requests latency budget burn for Kas Fleet Manager API p90 (current value: 609.3m)"
              runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/managed-services/sop#kafka-service-fleet-manager-latency"
  - interval: 1m
    input_series:
      # 0m    - 60m   all good
      # 60m   - 120m  10% of requests degraded to 1le bucket
      # 120m  - 150m  30% of requests degraded to 1le bucket
      - series: 'api_inbound_request_duration_bucket{job="kas-fleet-manager-metrics",namespace="managed-services-production",le="0.1",code="200"}'
        values: '0+1x60   61+9x60   601+7x30'
      - series: 'api_inbound_request_duration_bucket{job="kas-fleet-manager-metrics",namespace="managed-services-production",le="1",code="200"}'
        values: '0+1x60   61+1x60   121+3x30'
      - series: 'api_inbound_request_duration_count{job="kas-fleet-manager-metrics",namespace="managed-services-production"}'
        values: '0+1x60   61+10x60  661+10x30'
    alert_rule_test:
      - eval_time: 60m
        alertname: KasFleetManagerAPILatency2hto1dor6hto3dP90BudgetBurn
        exp_alerts: [ ]
      - eval_time: 120m
        alertname: KasFleetManagerAPILatency2hto1dor6hto3dP90BudgetBurn
        exp_alerts: [ ]
      - eval_time: 150m
        alertname: KasFleetManagerAPILatency2hto1dor6hto3dP90BudgetBurn
        exp_alerts:
          - exp_labels:
              alertname: KasFleetManagerAPILatency2hto1dor6hto3dP90BudgetBurn
              job: kas-fleet-manager-metrics
              latency: 0.1
              namespace: managed-services-production
              quantile: 90
              service: kas-fleet-manager
              severity: high
            exp_annotations:
              dashboard: "https://grafana.app-sre.devshift.net/d/Tbw1EgoMz/kas-fleet-manager-slos?orgId=1&var-datasource=app-sre-prod-04-prometheus"
              message: "High 1d/3d requests latency budget burn for Kas Fleet Manager API p90 (current value: 151.5m)"
              runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/managed-services/sop#kafka-service-fleet-manager-latency"
  - interval: 1m
    input_series:
      - series: 'api_inbound_request_duration_bucket{path="/api/kafkas_mgmt/v1/kafkas/-/metrics/federate",job="kas-fleet-manager-metrics",namespace="managed-services-production",le="5",code="200"}'
        values: '0+93x30   2790+100x30  5790+95x30'
      - series: 'api_inbound_request_duration_count{path="/api/kafkas_mgmt/v1/kafkas/-/metrics/federate",job="kas-fleet-manager-metrics",namespace="managed-services-production"}'
        values: '0+100x30   3000+100x30  6000+100x30'
    alert_rule_test:
      - eval_time: 30m #alert fires as rate of calls exceeding 5s is above burn rate (7% of calls)
        alertname: KasFleetManagerMetricsFederateLatency30mto6hP99BudgetBurn
        exp_alerts: 
          - exp_labels:
              alertname: KasFleetManagerMetricsFederateLatency30mto6hP99BudgetBurn
              job: kas-fleet-manager-metrics
              latency: 5
              namespace: managed-services-production
              quantile: 99
              service: kas-fleet-manager
              severity: high
            exp_annotations:
              dashboard: "https://grafana.app-sre.devshift.net/d/Tbw1EgoMz/kas-fleet-manager-slos?orgId=1&var-datasource=app-sre-prod-04-prometheus"
              message: "High 6h requests latency budget burn for Kas Fleet Manager /metrics/federate endpoint p99 (current value: 70m)"
              runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/managed-services/sop#kafka-service-fleet-manager-federate-metrics-endpoint-latency"
      - eval_time: 60m #no alert as rate has dropped below burn rate
        alertname: KasFleetManagerMetricsFederateLatency30mto6hP99BudgetBurn
        exp_alerts: [ ]
      - eval_time: 90m #no alert even though some calls are over SLO (rate has not exceeded burn rate)
        alertname: KasFleetManagerMetricsFederateLatency30mto6hP99BudgetBurn
        exp_alerts: [ ]
  - interval: 1m
    input_series:
      - series: 'api_inbound_request_duration_bucket{path="/api/kafkas_mgmt/v1/kafkas/-/metrics/federate",job="kas-fleet-manager-metrics",namespace="managed-services-production",le="5",code="200"}'
        values: '0+97x60   5820+500x60  35820+99x60'
      - series: 'api_inbound_request_duration_count{path="/api/kafkas_mgmt/v1/kafkas/-/metrics/federate",job="kas-fleet-manager-metrics",namespace="managed-services-production"}'
        values: '0+100x60   6000+500x60  36000+100x60'
    alert_rule_test:
      - eval_time: 60m #alert fires as rate of calls exceeding 5s is above burn rate (3% of calls)
        alertname: KasFleetManagerMetricsFederateLatency2hto1dor6hto3dP99BudgetBurn
        exp_alerts:
          - exp_labels:
              alertname: KasFleetManagerMetricsFederateLatency2hto1dor6hto3dP99BudgetBurn
              job: kas-fleet-manager-metrics
              latency: 5
              namespace: managed-services-production
              quantile: 99
              service: kas-fleet-manager
              severity: high
            exp_annotations:
              dashboard: "https://grafana.app-sre.devshift.net/d/Tbw1EgoMz/kas-fleet-manager-slos?orgId=1&var-datasource=app-sre-prod-04-prometheus"
              message: "High 1d/3d requests latency budget burn for Kas Fleet Manager /metrics/federate endpoint p99 (current value: 30m)"
              runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/managed-services/sop#kafka-service-fleet-manager-federate-metrics-endpoint-latency"
      - eval_time: 120m #no alert as rate has dropped below burn rate
        alertname: KasFleetManagerMetricsFederateLatency2hto1dor6hto3dP99BudgetBurn
        exp_alerts: [ ]
      - eval_time: 180m #no alert even though some calls are over SLO (rate has not exceeded burn rate)
        alertname: KasFleetManagerMetricsFederateLatency2hto1dor6hto3dP99BudgetBurn
        exp_alerts: [ ]
  - interval: 1m
    input_series:
      - series: 'api_inbound_request_duration_bucket{path="/api/kafkas_mgmt/v1/kafkas/-/metrics/federate",job="kas-fleet-manager-metrics",namespace="managed-services-production",le="2",code="200"}'
        values: '0+40x30   1200+100x30  4200+41x30'
      - series: 'api_inbound_request_duration_count{path="/api/kafkas_mgmt/v1/kafkas/-/metrics/federate",job="kas-fleet-manager-metrics",namespace="managed-services-production"}'
        values: '0+100x30   3000+100x30  6000+100x30'
    alert_rule_test:
      - eval_time: 30m #alert fires as rate of calls exceeding 2s is above burn rate (60% of calls)
        alertname: KasFleetManagerMetricsFederateLatency30mto6hP90BudgetBurn
        exp_alerts:
          - exp_labels:
              alertname: KasFleetManagerMetricsFederateLatency30mto6hP90BudgetBurn
              job: kas-fleet-manager-metrics
              latency: 2
              namespace: managed-services-production
              quantile: 90
              service: kas-fleet-manager
              severity: high
            exp_annotations:
              dashboard: "https://grafana.app-sre.devshift.net/d/Tbw1EgoMz/kas-fleet-manager-slos?orgId=1&var-datasource=app-sre-prod-04-prometheus"
              message: "High 1h/6h requests latency budget burn for Kas Fleet Manager /metrics/federate endpoint p90 (current value: 600m)"
              runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/managed-services/sop#kafka-service-fleet-manager-federate-metrics-endpoint-latency"
      - eval_time: 60m #no alert as rate has dropped below burn rate
        alertname: KasFleetManagerMetricsFederateLatency30mto6hP90BudgetBurn
        exp_alerts: [ ]
      - eval_time: 90m #no alert even though some calls are over SLO (rate has not exceeded burn rate)
        alertname: KasFleetManagerMetricsFederateLatency30mto6hP90BudgetBurn
        exp_alerts: [ ]
  - interval: 1m
    input_series:
      - series: 'api_inbound_request_duration_bucket{path="/api/kafkas_mgmt/v1/kafkas/-/metrics/federate",job="kas-fleet-manager-metrics",namespace="managed-services-production",le="2",code="200"}'
        values: '0+89x60   5340+100x60  11340+90x60'
      - series: 'api_inbound_request_duration_count{path="/api/kafkas_mgmt/v1/kafkas/-/metrics/federate",job="kas-fleet-manager-metrics",namespace="managed-services-production"}'
        values: '0+100x60   6000+100x60  12000+100x60'
    alert_rule_test:
      - eval_time: 60m #alert fires as rate of calls exceeding 2s is above burn rate (10% of calls)
        alertname: KasFleetManagerMetricsFederateLatency2hto1dor6hto3dP90BudgetBurn
        exp_alerts:
          - exp_labels:
              alertname: KasFleetManagerMetricsFederateLatency2hto1dor6hto3dP90BudgetBurn
              job: kas-fleet-manager-metrics
              latency: 2
              namespace: managed-services-production
              quantile: 90
              service: kas-fleet-manager
              severity: high
            exp_annotations:
              dashboard: "https://grafana.app-sre.devshift.net/d/Tbw1EgoMz/kas-fleet-manager-slos?orgId=1&var-datasource=app-sre-prod-04-prometheus"
              message: "High 1d/3d requests latency budget burn for Kas Fleet Manager /metrics/federate endpoint p90 (current value: 110m)"
              runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/managed-services/sop#kafka-service-fleet-manager-federate-metrics-endpoint-latency"
      - eval_time: 120m #no alert as rate has dropped below burn rate
        alertname: KasFleetManagerMetricsFederateLatency2hto1dor6hto3dP90BudgetBurn
        exp_alerts: [ ]
      - eval_time: 180m #no alert even though some calls are over SLO (rate has not exceeded burn rate)
        alertname: KasFleetManagerMetricsFederateLatency2hto1dor6hto3dP90BudgetBurn
        exp_alerts: [ ]
