---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: app-sre
    role: alert-rules
  name: glitchtip-production
spec:
  groups:
  - name: glitchtip-production-rules
    rules:
    - alert: PrometheusFailedToScrapeGlitchTipWeb
      annotations:
        message: "Prometheus failed to scrape glitchtip-web."
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/glitchtip/sops/prometheus-failed-to-scrape-glitchtip-web.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/GRUqV30Vk/glitchtip-dashboard?orgId=1"
      # Fire when prometheus failed to scrape glitchtip-web service. The right side of the and operator
      # ensures that there is indeed an unscrapable prometheus target so that when up metric is missing
      # for some reason (e.g. node restart) then we don't get a false-positive alert.
      expr: |
        absent(up{job="glitchtip-web"} == 1) and count(up{job="glitchtip-web"} == 0) > 0
      for: 1m
      labels:
        service: glitchtip
        severity: medium

    - alert: GlitchtipWebDown
      expr: sum by(container) (kube_pod_container_status_ready{namespace=~"glitchtip-.+", container="web"}) < 1
      for: 1m
      labels:
        service: glitchtip
        severity: critical
      annotations:
        message: "Glitchtip {{ $labels.container }} deployment is down."
        dashboard: "https://grafana.app-sre.devshift.net/d/GRUqV30Vk/glitchtip-dashboard?orgId=1"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/glitchtip/sops/glitchtip-pod-ready.md"

    - alert: GlitchtipWorkerDown
      expr: sum by(container) (kube_pod_container_status_ready{namespace=~"glitchtip-.+", container="worker"}) < 1
      for: 1m
      labels:
        service: glitchtip
        severity: critical
      annotations:
        message: "Glitchtip {{ $labels.container }} deployment is down."
        dashboard: "https://grafana.app-sre.devshift.net/d/GRUqV30Vk/glitchtip-dashboard?orgId=1"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/glitchtip/sops/glitchtip-pod-ready.md"

    - alert: GlitchtipBeatDown
      expr: sum by(container) (kube_pod_container_status_ready{namespace=~"glitchtip-.+", container="beat"}) < 1
      for: 1m
      labels:
        service: glitchtip
        severity: critical
      annotations:
        message: "Glitchtip {{ $labels.container }} deployment is down."
        dashboard: "https://grafana.app-sre.devshift.net/d/GRUqV30Vk/glitchtip-dashboard?orgId=1"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/glitchtip/sops/glitchtip-pod-ready.md"

    - alert: GlitchtipDeploymentUnderReplicated
      expr: kube_deployment_status_replicas_unavailable{namespace=~"glitchtip-.+"} > 0
      for: 5m
      labels:
        service: glitchtip
        severity: medium
      annotations:
        message: "Glitchtip deployment {{ $labels.deployment }} has {{ $value }} pod(s) not running."
        dashboard: "https://grafana.app-sre.devshift.net/d/GRUqV30Vk/glitchtip-dashboard?orgId=1"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/glitchtip/sops/glitchtip-pod-ready.md"

    - alert: GlitchtipErrorRateHigh
      expr: |
        (
          sum by (route)(rate(haproxy_backend_http_responses_total{route="glitchtip-route",code="5xx"}[5m]))
          /
          sum by (route)(rate(haproxy_backend_http_responses_total{route="glitchtip-route"}[5m]))
        ) * 100 > 5
      for: 5m
      labels:
        service: glitchtip
        severity: critical
      annotations:
        message: "Glitchtip route {{ $labels.route }} is failing to handle {{$value | humanize}}% of the requests."
        dashboard: "https://grafana.app-sre.devshift.net/d/GRUqV30Vk/glitchtip-dashboard?orgId=1"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/tree/master/docs/glitchtip/sops/glitchtip-error-500.md"
