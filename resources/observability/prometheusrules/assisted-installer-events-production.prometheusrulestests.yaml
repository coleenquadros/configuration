$schema: /app-interface/prometheus-rule-test-1.yml

rule_files:
- /observability/prometheusrules/assisted-installer-events-production.prometheusrules.yaml

evaluation_interval: 1m

tests:
- interval: 5m
  input_series:
  - series: kube_pod_container_status_restarts_total{container="assisted-events-scrape"}
    values: 0 0 0 0 0 0 1 2 3 4 5 6 6 7 8 9 9 9 9 9 9 9 9 9

  alert_rule_test:
  # Test the no alert case
  - eval_time: 30m
    alertname: Assisted Installer Events - Too many restarts - Production
    exp_alerts:
  # Test alert for continuously growing
  - eval_time: 60m
    alertname: Assisted Installer Events - Too many restarts - Production
    exp_alerts:
    - exp_labels:
        service: assisted-installer
        severity: info
        container: assisted-events-scrape
      exp_annotations:
        message: "Too many restarts for assisted-installer-events pod"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/assisted-installer/sop#assisted-installer-events-is-down"
  # Test alert for sparse growth
  - eval_time: 80m
    alertname: Assisted Installer Events - Too many restarts - Production
    exp_alerts:
    - exp_labels:
        service: assisted-installer
        severity: info
        container: assisted-events-scrape
      exp_annotations:
        message: "Too many restarts for assisted-installer-events pod"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/assisted-installer/sop#assisted-installer-events-is-down"
  # Test the no alert when restarts stabilize
  - eval_time: 120m
    alertname: Assisted Installer Events - Too many restarts - Production
    exp_alerts:

# No CPU activity test
- interval: 15m
  input_series:
  - series: container_cpu_usage_seconds_total{container="assisted-events-scrape",pod="assisted-events-scrape-85db75b79b-dfh7j"}
    values: 0 120.123 240.456 360.789 360.789 360.789 360.789 360.789 360.789 365.789 370.789 380.789 390.789 400.789

  alert_rule_test:
  # Test the no alert case
  - eval_time: 60m
    alertname: Assisted Installer Events - No CPU activity detected - Production
    exp_alerts:
  # Test the no alert, but about to fire
  - eval_time: 105m
    alertname: Assisted Installer Events - No CPU activity detected - Production
    exp_alerts:
  # Alert when no CPU activity
  - eval_time: 120m
    alertname: Assisted Installer Events - No CPU activity detected - Production
    exp_alerts:
    - exp_labels:
        pod: assisted-events-scrape-85db75b79b-dfh7j
        service: assisted-installer
        severity: info
      exp_annotations:
        message: "Assisted Installer Events - No CPU activity in the last hour detected for pod assisted-events-scrape-85db75b79b-dfh7j"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/assisted-installer/sop#assisted-installer-events-no-cpu-activity"
  # No alert as CPU activity came back
  - eval_time: 135m
    alertname: Assisted Installer Events - No CPU activity detected - Production
    exp_alerts:

# Events ingestion tests
- interval: 15m
  input_series:
  - series: assisted_installer_events_max_event_id{container="prometheus-postgres-exporter", endpoint="http", instance="10.131.2.123:9187", job="assisted-installer-prometheus-postgres-exporter", namespace="assisted-installer-production", pod="assisted-installer-prometheus-postgres-exporter-5d495cf6fftm5x6", server="assisted-installer.rds.awshost.com:5432", service="assisted-installer-prometheus-postgres-exporter"}
    values: 1+1x16 4+1x96
  - series: elasticsearch_indices_shards_docs{cluster="ai-production-es", container="exporter", endpoint="http", index="assisted-service-events-v3", instance="10.130.5.220:9108", job="prometheus-opensearch-exporter", namespace="assisted-installer-production", node="0swM1yuXSkiflKGaSwWgYw", pod="prometheus-opensearch-exporter-59f7dc576f-c6nl6", primary="true", service="prometheus-opensearch-exporter", shard="0"}
    values: 1+1x16 4+0x48 4+1x48
  # let's make sure we don't pick up replication as events
  - series: elasticsearch_indices_shards_docs{cluster="ai-production-es", container="exporter", endpoint="http", index="assisted-service-events-v3", instance="10.130.5.220:9108", job="prometheus-opensearch-exporter", namespace="assisted-installer-production", node="0swM1yuXSkiflKGaSwWgYw", pod="prometheus-opensearch-exporter-59f7dc576f-c6nl6", primary="false", service="prometheus-opensearch-exporter", shard="0"}
    values: 1+1x16 4+0x48 4+1x48
  alert_rule_test:
  # first hour growth at the same rate, second hour elasticsearch stops and events keep going: alert
  - eval_time: 13h0m
    alertname: Assisted Installer Events - Assisted service events ingestion greatly reduced - Production
    exp_alerts:
    - exp_labels:
        service: assisted-installer
        severity: info
        namespace: assisted-installer-production
      exp_annotations:
        message: "Assisted Installer Events - Assisted service events ingestion greatly reduced for index assisted-service-events-v3"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/assisted-installer/sop#assisted-installer-events-ingestion-reduced"
  # No alert after 1h as both metrics growing at the same rate
  - eval_time: 1h
    alertname: Assisted Installer Events - Assisted service events ingestion greatly reduced - Production
    exp_alerts:
  # No alert after 6h as elasticsearch got back writing events and it's now growing at the same rate
  - eval_time: 6h
    alertname: Assisted Installer Events - Assisted service events ingestion greatly reduced - Production
    exp_alerts:
# Events ingestion tests for .events (CCX export)
- interval: 15m
  input_series:
  - series: assisted_installer_events_max_event_id{container="prometheus-postgres-exporter", endpoint="http", instance="10.131.2.123:9187", job="assisted-installer-prometheus-postgres-exporter", namespace="assisted-installer-production", pod="assisted-installer-prometheus-postgres-exporter-5d495cf6fftm5x6", server="assisted-installer.rds.awshost.com:5432", service="assisted-installer-prometheus-postgres-exporter"}
    values: 1+1x4 4+1x24
  - series: elasticsearch_indices_shards_docs{cluster="ai-production-es", container="exporter", endpoint="http", index=".events", instance="10.130.5.220:9108", job="prometheus-opensearch-exporter", namespace="assisted-installer-production", node="0swM1yuXSkiflKGaSwWgYw", pod="prometheus-opensearch-exporter-59f7dc576f-c6nl6", primary="true", service="prometheus-opensearch-exporter", shard="0"}
    values: 1+1x4 4+0x12 4+1x12
  # let's make sure we don't pick up replication as events
  - series: elasticsearch_indices_shards_docs{cluster="ai-production-es", container="exporter", endpoint="http", index=".events", instance="10.130.5.220:9108", job="prometheus-opensearch-exporter", namespace="assisted-installer-production", node="0swM1yuXSkiflKGaSwWgYw", pod="prometheus-opensearch-exporter-59f7dc576f-c6nl6", primary="false", service="prometheus-opensearch-exporter", shard="0"}
    values: 1+1x4 4+0x12 4+1x12
  alert_rule_test:
  # first hour growth at the same rate, second hour elasticsearch stops and events keep going: alert
  - eval_time: 3h15m
    alertname: Assisted Installer Events - Assisted service events ingestion greatly reduced - Production
    exp_alerts:
    - exp_labels:
        service: assisted-installer
        severity: info
        namespace: assisted-installer-production
      exp_annotations:
        message: "Assisted Installer Events - Assisted service events ingestion greatly reduced for index .events (CCX export)"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/docs/assisted-installer/sop#assisted-installer-events-ingestion-reduced"
  # No alert after 1h as both metrics growing at the same rate
  - eval_time: 1h
    alertname: Assisted Installer Events - Assisted service events ingestion greatly reduced - Production
    exp_alerts:
  # No alert after 6h as elasticsearch got back writing events and it's now growing at the same rate
  - eval_time: 6h
    alertname: Assisted Installer Events - Assisted service events ingestion greatly reduced - Production
    exp_alerts:
