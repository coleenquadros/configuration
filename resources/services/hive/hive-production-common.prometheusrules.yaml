---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: app-sre
    role: alert-rules
  name: hive-production-{{{shard_name}}}
spec:
  groups:
  - name: hive-production-{{{shard_name}}}
    rules:
    # Hive team + AppSRE care about these:
    - alert: HiveControllersDown - production - {{{shard_name}}}
      annotations:
        message: "No targets found for hive-controllers in namespace {{ $labels.namespace }}. hive-controllers is down."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/HiveControllerDown.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="hive-controllers"} == 1)
      for: 5m
      labels:
        service: hive
        severity: critical
        team: appsre
    - alert: HiveClusterSyncDown - production - {{{shard_name}}}
      annotations:
        message: "No targets found for hive-clustersync in namespace {{ $labels.namespace }}. hive-clustersync is down."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/HiveControllerDown.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="hive-clustersync"} == 1)
      for: 5m
      labels:
        service: hive
        severity: critical
        team: appsre
    - alert: HiveOperatorDown - production - {{{shard_name}}}
      annotations:
        message: "No targets found for hive-operator in namespace {{ $labels.namespace }}. hive-operator is down."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/HiveOperatorDown.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="openshift-customer-monitoring/hive-operator"} == 1)
      for: 10m
      labels:
        service: hive
        severity: critical
        team: appsre
    - alert: HiveDeploymentFailed - production - {{{shard_name}}}
      annotations:
        message: "hive deployment has failed. Condition/Reason: {{ $labels.condition }} / {{ $labels.reason }}."
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/HiveDeploymentFailed.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        hive_hiveconfig_conditions{condition="Ready"} == 0
      for: 5m
      labels:
        service: hive
        severity: critical
        team: appsre
    - alert: InstallJobDelayHigh - production - {{{shard_name}}}
      annotations:
        message: "Time to start an install job is taking greater than 5 minutes"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/InstallJobDelayHigh.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        rate(hive_cluster_deployment_install_job_delay_seconds_sum{job="hive-controllers"}[6h])
        /
        rate(hive_cluster_deployment_install_job_delay_seconds_count{job="hive-controllers"}[6h]) > 300
      for: 10m
      labels:
        service: hive
        severity: high
    - alert: ControllerErrorsHigh - production - {{{shard_name}}}
      annotations:
        message: "{{ $labels.controller }} controller is reporting a high error rate: {{ $value }}/s"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/ControllerErrorsHigh.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        rate(controller_runtime_reconcile_errors_total{job="hive-controllers"}[15m]) > 1
      for: 20m
      labels:
        service: hive
        severity: critical
        team: appsre
    - alert: LocalKubeClientRequestsHigh - POST requests - production - {{{shard_name}}}
      annotations:
        message: "detected {{ $value }} {{ $labels.resource }} local POST API requests per second from controller {{ $labels.controller }}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/KubeClientRequestsHigh.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        rate(hive_kube_client_requests_total{job="hive-controllers",remote="false",method="POST"}[5m]) > 15
      for: 1m
      labels:
        service: hive
        severity: critical
        team: appsre
    - alert: LocalKubeClientRequestsHigh - non-POST requests - production - {{{shard_name}}}
      annotations:
        message: "detected {{ $value }} {{ $labels.resource }} local non-POST API requests per second from controller {{ $labels.controller }}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/KubeClientRequestsHigh.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        rate(hive_kube_client_requests_total{job="hive-controllers",remote="false",method!~"POST"}[5m]) > 15
      for: 1m
      labels:
        service: hive
        severity: medium
        team: appsre
    # end hive + AppSRE section

    # SREP cares about the below alerts
    - alert: ClusterProvisioningDelay - production
      annotations:
        message: "cluster {{ $labels.cluster_deployment }} in namespace {{ $labels.exported_namespace }} provisioning taking over 2 hours. Condition/Reason: {{ $labels.condition }} / {{ $labels.reason }}. SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterProvisioningFailure.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      # filter out all OCM3xxx codes except these --
      # they require SRE response because they indicate provisioning infra problems:
      # AWSUnableToFindMatchingRouteTable
      # DNSAlreadyExists
      # NoMatchingRoute53Zone
      # GCPInvalidProjectID
      expr: |
        (max without (instance,pod,cluster_type) (hive_cluster_deployment_provision_underway_seconds{job="hive-controllers",reason!~"AWSAPIRateLimitExceeded|AWSEC2QuotaExceeded|AWSVPCLimitExceeded|S3BucketsLimitExceeded|InvalidCredentials|EIPAddressLimitExceeded|NoWorkerNodes|AWSNATGatewayLimitExceeded|InvalidInstallConfigSubnet|PendingVerification|GCPInstanceTypeNotFound|GCPPreconditionFailed|GCPQuotaSSDTotalGBExceeded|GCPComputeQuotaExceeded|GCPServiceAccountQuotaExceeded|InvalidAWSTags|MissingPublicSubnetForZone|PrivateSubnetInMultipleZones|ErrorDeletingIAMRole|AWSSubnetDoesNotExist|AWSInsufficientPermissions|VcpuLimitExceeded|UserInitiatedShutdown|LoadBalancerLimitExceeded|AWSAccessDeniedSLR|ProxyTimeout|ProxyInvalidCABundle|ErrorCreatingNetworkLoadBalancer|NATGatewayFailed"} ) /3600) > 2
        and
        (max without (instance,pod,cluster_type) (hive_cluster_deployment_provision_underway_seconds{job="hive-controllers",reason!~"AWSAPIRateLimitExceeded|AWSEC2QuotaExceeded|AWSVPCLimitExceeded|S3BucketsLimitExceeded|InvalidCredentials|EIPAddressLimitExceeded|NoWorkerNodes|AWSNATGatewayLimitExceeded|InvalidInstallConfigSubnet|PendingVerification|GCPInstanceTypeNotFound|GCPPreconditionFailed|GCPQuotaSSDTotalGBExceeded|GCPComputeQuotaExceeded|GCPServiceAccountQuotaExceeded|InvalidAWSTags|MissingPublicSubnetForZone|PrivateSubnetInMultipleZones|ErrorDeletingIAMRole|AWSSubnetDoesNotExist|AWSInsufficientPermissions|VcpuLimitExceeded|UserInitiatedShutdown|LoadBalancerLimitExceeded|AWSAccessDeniedSLR|ProxyTimeout|ProxyInvalidCABundle|ErrorCreatingNetworkLoadBalancer|NATGatewayFailed"} ) /3600) < 168
      for: 10m
      labels:
        service: hive
        severity: critical
        team: srep
    - alert: ClusterDeprovisioningDelay - production - {{{shard_name}}}
      annotations:
        message: "cluster {{ $labels.cluster_deployment }} in namespace {{ $labels.namespace }} deprovision taking over 6 hours"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/ClusterDeprovisioningDelay.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        (hive_cluster_deployment_deprovision_underway_seconds{job="hive-controllers"} / 3600 ) > 6
      for: 10m
      labels:
        severity: medium
        service: hive
        team: srep
    - alert: SelectorSyncsetApplyFailures - production - {{{shard_name}}}
      annotations:
        message: "The SelectorSyncset {{ $labels.name }} has more than 1 unapplied instances for 15 minutes"
        runbook: "https://github.com/openshift/ops-sop/tree/master/v4/alerts/SelectorSyncsetApplyFailures.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        hive_selectorsyncset_clusters_unapplied_total{job="hive-controllers"} > 0
      for: 15m
      labels:
        service: hive
        severity: medium
        team: srep
    - alert: SyncsetPausedTooLong - production - {{{shard_name}}}
      annotations:
        message: "Cluster {{ $labels.cluster_deployment }} in namespace {{ $labels.exported_namespace }} has paused Hive syncrhonization for longer than 48 hours"
        runbook: "https://github.com/openshift/ops-sop/tree/master/v4/alerts/SyncsetPausedTooLong.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        hive_cluster_deployment_syncset_paused{cluster_type="managed"} > 0
      for: 48h
      labels:
        service: hive
        severity: medium
        team: srep
    - alert: AWSAccountOperator AccountPool Depletion - {{{shard_name}}}
      annotations:
        message: "AWS Account Operator's AccountPool is nearing depletion."
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/AWSAccountOperatorAccountPoolDepleted.md"
        dashboard: "https://grafana.app-sre.devshift.net/explore?orgId=1&left=%5B%22now-7d%22,%22now%22,%22{{{shard_name}}}-prometheus%22,%7B%22expr%22:%22aws_account_operator_available_non_ccs_accounts%20%2B%20aws_account_operator_accounts_progressing%20without%20(instance,%20pod)%5Cn%5Cn%22,%22requestId%22:%22Q-8ea2341f-51e8-4e34-aa3a-6469db5fcf6d-0A%22,%22format%22:%22time_series%22,%22instant%22:false,%22refId%22:%22A%22,%22exemplar%22:true,%22range%22:true%7D%5D"
      expr: |
        aws_account_operator_available_non_ccs_accounts + aws_account_operator_accounts_progressing < (aws_account_operator_account_pool_size / 10)
      labels:
        service: srep
        severity: critical
        team: srep
    - alert: AWSAccountOperator Target Down - production - {{{shard_name}}}
      annotations:
        message: "No targets found for AWS Account Operator on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/SREOperators.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="aws-account-operator"} == 1)
      for: 10m
      labels:
        service: srep
        severity: critical
        team: srep
    - alert: CertmanOperator Target Down - production - {{{shard_name}}}
      annotations:
        message: "No targets found for Certman Operator on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/SREOperators.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="certman-operator"} == 1)
      for: 10m
      labels:
        service: srep
        severity: critical
        team: srep
    - alert: DeadMansSnitchOperator Target Down - production - {{{shard_name}}}
      annotations:
        message: "No targets found for Dead Mans Snitch Operator on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/SREOperators.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="deadmanssnitch-operator"} == 1)
      for: 10m
      labels:
        service: srep
        severity: critical
        team: srep
    - alert: PagerDutyOperator Target Down - production - {{{shard_name}}}
      annotations:
        message: "No targets found for PagerDuty Operator on hive-{{{shard_name}}}"
        runbook: "https://github.com/openshift/hive-sops/blob/master/sop/SREOperators.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/hivemetrics/hive-metrics?orgId=1&var-datasource={{{grafana_datasource}}}&var-instance=hive-controllers-{{{shard_name}}}"
      expr: |
        absent(up{job="pagerduty-operator"} == 1)
      for: 10m
      labels:
        service: srep
        severity: critical
        team: srep
    - alert: LetsEncryptIssuanceRateLimit - devshift.org - {{{shard_name}}}
      annotations:
        message: "Certman Operator is about to reach account limit for issued certificate in the last 7 days - devshift.org domain"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/howto/lets-encrypt-rate-limit-adjustment.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/sd-scale/service-delivery-scale?orgId=1"
      expr: |
        certman_operator_certs_in_last_week_devshift_org{job="certman-operator"} > {{{certman_operator_devshift_org_issuance_rate_threshold}}}
      labels:
        service: srep
        severity: critical
        team: srep
    - alert: LetsEncryptIssuanceRateLimit - openshiftapps.com - {{{shard_name}}}
      annotations:
        message: "Certman Operator is about to reach account limit for issued certificate in the last 7 days - openshiftapps.com domain"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/howto/lets-encrypt-rate-limit-adjustment.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/sd-scale/service-delivery-scale?orgId=1"
      expr: |
        certman_operator_certs_in_last_week_openshift_apps_com{job="certman-operator"} > {{{certman_operator_openshift_apps_com_issuance_rate_threshold}}}
      labels:
        service: srep
        severity: critical
        team: srep
    - alert: CertmanCertExpires5Days - {{{shard_name}}}
      annotations:
        message: "certificate {{ $labels.cn }} from hive-{{{shard_name}}} expires in 5 days and has not renewed"
        runbook: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/CertmanCertExpires5Days.md"
      expr: |
        certman_operator_certificate_valid_duration_days <= 5
      labels:
        service: srep
        severity: critical
        team: srep
