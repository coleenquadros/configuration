$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: app-sre
    role: alert-rules
  name: rhsm
spec:
  groups:
  - name: rhsm-insights-prod
    rules:

    - alert: Rhsm5xx
      expr: >
        (sum(rate(api_3scale_gateway_api_status{exported_service="rhsm-subscriptions", status="5xx"}[15m]))
        /
        sum(rate(api_3scale_gateway_api_status{exported_service="rhsm-subscriptions"}[15m]))) * 100 > 10
      for: 5m
      labels:
        # Move to a "high" priority once on-boarding is completed
        severity: medium
        service: insights
        env: prod
        app_team: rhsm
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/lkPhH-1Zk/subscription-watch?orgId=1"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/rhsm-prod/deployments"
        message: "RHSM: more than 10% requests in prod returned HTTP 5XX in last 15 minutes"
        runbook: 'https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/console.redhat.com/app-sops/rhsm/Rhsm5xx.md'

    - alert: RhsmLatency
      # The sum of the rate of requests over 15 minutes within the time bucket with the upper limit of 2000ms
      # is less than 90% of the sum of the rate of total rhsm-subscriptions requests happening over 15 minutes.
      # Which means that we have at least 10% of requests slower than 2000ms for at least 5 minutes
      expr: |
          sum(rate(api_3scale_gateway_api_time_bucket{exported_service="rhsm-subscriptions", le="2000.0"}[15m]))
          /
          sum(rate(api_3scale_gateway_api_time_count{exported_service="rhsm-subscriptions"}[15m])) < 0.9
      for: 5m
      labels:
        # Move to a "high" priority once on-boarding is completed
        severity: medium
        service: insights
        env: prod
        app_team: rhsm
        exported_service: rhsm-subscriptions
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/lkPhH-1Zk/subscription-watch?orgId=1"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/rhsm-prod/deployments"
        message: "RHSM: slow responses observed; more than 10% of requests with response time > 2 seconds for the last 15 minutes in prod"
        runbook: 'https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/console.redhat.com/app-sops/rhsm/RhsmLatency.md'

    - alert: SplunkHECFailing
      expr: |
        rate(splunk_hec_message_failure_total[10m]) > 0
      for: 5m
      labels:
        severity: medium
        service: insights
        env: prod
        app_team: rhsm
      annotations:
        message: "Splunk HEC logging is failing for {{ $labels.namespace }}"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/rhsm-prod/deployments"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/console.redhat.com/app-sops/rhsm/SplunkHecFailure.md"
        dashboard: "https://grafana.app-sre.devshift.net/d/lkPhH-1Zk/subscription-watch?orgId=1&viewPanel=88"

    - alert: RhsmRdsFreeSpaceLow
      expr: |
        predict_linear(sum by (exported_instance,mount_point) (rdsosmetrics_fileSys_usedPercent{exported_instance="rhsm-prod",mount_point="/rdsdbdata"})[4d:30m], 3600 * 24 * 14) > 100
      labels:
        severity: medium
        service: insights
        env: prod
        app_team: rhsm
      annotations:
        message: "DB instance {{ $labels.exported_instance }} is about to run out of storage within 2 weeks."
        link_url: "https://console.aws.amazon.com/rds/home?region=us-east-1#database:id=rhsm-prod;is-cluster=false;tab=monitoring"
        dashboard: "https://grafana.app-sre.devshift.net/d/lkPhH-1Zk/subscription-watch?orgId=1&viewPanel=52"
        runbook: 'https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/console.redhat.com/app-sops/rhsm/RhsmRdsFreeSpaceLow.md'

    - alert: SwatchKubePodCrashLooping
      expr: |
        sum(increase(kube_pod_container_status_restarts_total{namespace="rhsm-prod"}[3h])) > 3
      for: 15m
      labels:
        severity: high
        service: insights
        env: prod
        app_team: rhsm
        exported_service: rhsm-subscriptions
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/lkPhH-1Zk/subscription-watch?orgId=1"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/rhsm-prod/deployments"
        message: "RHSM: more than 3 restarts in prod happened in that last 3 hours"
        runbook: 'https://gitlab.cee.redhat.com/service/app-interface/blob/master/docs/app-sre/sop/kubernetes/KubePodCrashLooping.md'

    - alert: RhsmJobFailedToComplete
      expr: |
        sum by (job_name) (kube_job_status_failed{namespace="rhsm-prod",job_name!~"swatch-metrics.*"}) - sum by (job_name) (kube_job_status_failed{namespace="rhsm-prod",job_name!~"swatch-metrics.*"} offset 1h) > 0
      labels:
        severity: high
        service: insights
        env: prod
        app_team: rhsm
        exported_service: rhsm-subscriptions
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/lkPhH-1Zk/subscription-watch?orgId=1"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/rhsm-prod/deployments"
        message: "RHSM: {{ $labels.job_name }} job failed in production"
        runbook: "https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/console.redhat.com/app-sops/rhsm/RhsmJobFailedToComplete.md"

    - alert: RhsmConduitValidationErrors
      expr: sum(increase(inventory_ingress_add_host_failures_total{cause="ValidationException", reporter="rhsm-conduit", namespace=~"host-inventory-.*"}[5m])) > 0
      labels:
        severity: medium
        service: insights
        env: prod
        app_team: rhsm
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/EiIhtC0Wa/inventory?orgId=1&var-datasource=crcp01ue1-prometheus&refresh=5m&viewPanel=68
        link_url: https://kibana.apps.crcp01ue1.o9m8.p1.openshiftapps.com/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-12h,mode:quick,to:now))&_a=(columns:!(_source),filters:!(),index:'43c5fed0-d5ce-11ea-b58c-a7c95afd7a5d',interval:auto,query:(language:lucene,query:'@log_stream:%20%22host-inventory-mq-*%22%20AND%20message:%20%22Validation%20error%20while%20adding%20or%20updating%20host%20%22%20AND%20host.reporter:%20%22rhsm-conduit%22'),sort:!('@timestamp',desc))
        message: 'Validation errors for rhsm-conduit messages in host-inventory-prod'
        runbook: 'https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/console.redhat.com/app-sops/rhsm/RhsmConduitValidationErrors.md'

    - alert: TalliesWillMissWindow
      # Predicts when tally is expected to leave too many tasks unprocessed in an hour window
      # based on the last 10 minutes, do a linear regression and if the group lag will be greater than 0 for an hour
      expr: sum(predict_linear(kafka_consumergroup_group_lag{topic='platform.rhsm-subscriptions.tasks'}[10m], 3600)) > 0
      for: 1h
      labels:
        severity: medium
        service: insights
        env: prod
        app_team: rhsm
      annotations:
        dashboard: "https://grafana.app-sre.devshift.net/d/lkPhH-1Zk/subscription-watch?orgId=1"
        link_url: "https://console-openshift-console.apps.crcp01ue1.o9m8.p1.openshiftapps.com/k8s/ns/rhsm-prod/deployments"
        message: "tally predicted to not finish processing in a timely matter"
        runbook: 'https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/docs/console.redhat.com/app-sops/rhsm/TalliesWillMissWindow.md'

